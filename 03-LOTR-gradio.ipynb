{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def yes_man(message, history):\n",
    "    if message.endswith(\"?\"):\n",
    "        return \"Yes\"\n",
    "    else:\n",
    "        return \"No\"\n",
    "\n",
    "gr.ChatInterface(\n",
    "    yes_man,\n",
    "    textbox=gr.Textbox(placeholder=\"Ask me a question about the LOTR\", container=False, scale=7),\n",
    "    title=\"Yes Man\",\n",
    "    description=\"Ask Yes Man any question\",\n",
    "    theme=\"soft\",\n",
    "    examples=[\"Hello\", \"Am I cool?\", \"Are tomatoes vegetarian?\"],\n",
    "    cache_examples=True,\n",
    "    retry_btn=None,\n",
    "    undo_btn=\"Delete Previous\",\n",
    "    clear_btn=\"Clear\",\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "\n",
    "from llama_index.core import set_global_tokenizer\n",
    "\n",
    "set_global_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\").encode)\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"index\")\n",
    "\n",
    "# load index\n",
    "index = load_index_from_storage(storage_context, embed_model= embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.llms.llama_cpp.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")\n",
    "\n",
    "\n",
    "model_url = \"https://huggingface.co/TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF/resolve/main/capybarahermes-2.5-mistral-7b.Q4_0.gguf\"\n",
    "\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    model_url=model_url,\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=None,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\n",
    "        \"n_gpu_layers\": -1,\n",
    "        \"torch_dtype\": torch.float16,\n",
    "        \"load_in_8bit\": True,\n",
    "    },\n",
    "    # transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    chat_mode=\"condense_question\",\n",
    "    streaming=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   19003.12 ms\n",
      "llama_print_timings:      sample time =      34.67 ms /   137 runs   (    0.25 ms per token,  3951.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =   89086.94 ms /  1943 tokens (   45.85 ms per token,    21.81 tokens per second)\n",
      "llama_print_timings:        eval time =   19367.95 ms /   136 runs   (  142.41 ms per token,     7.02 tokens per second)\n",
      "llama_print_timings:       total time =  108787.03 ms /  2079 tokens\n"
     ]
    }
   ],
   "source": [
    "response =  query_engine.query(\"what happens to frodo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Frodo becomes less active in Shire affairs after his return from Middle Earth, and he gradually becomes more reclusive. In the text, it is mentioned that Frodo drops out of sight and people don't know much about his deeds and adventures. He also falls ill twice but manages to conceal it. In March 1421, Frodo gets sick again, but Sam has other things to think about as Sam's wife Rosie gives birth to their child on the same day. Frodo plans a trip with Sam to Bilbo's birthday celebration, indicating that he wants to see his old friend one last time. [</INST]\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7870\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   19003.12 ms\n",
      "llama_print_timings:      sample time =      27.12 ms /   115 runs   (    0.24 ms per token,  4240.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =   70762.24 ms /  1762 tokens (   40.16 ms per token,    24.90 tokens per second)\n",
      "llama_print_timings:        eval time =   16320.56 ms /   114 runs   (  143.16 ms per token,     6.99 tokens per second)\n",
      "llama_print_timings:       total time =   87366.99 ms /  1876 tokens\n"
     ]
    }
   ],
   "source": [
    "def predict(message, history):\n",
    "    history = []\n",
    "    for human, assistant in history:\n",
    "        history.append({\"role\": \"user\", \"content\": human })\n",
    "        history.append({\"role\": \"assistant\", \"content\":assistant})\n",
    "    history.append({\"role\": \"user\", \"content\": message})\n",
    "    response = query_engine.query(str(history))\n",
    "    return str(response.response)\n",
    "gr.ChatInterface(predict).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine.query(str([{'role': 'user', 'content': 'who died'}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   19003.12 ms\n",
      "llama_print_timings:      sample time =       5.19 ms /    24 runs   (    0.22 ms per token,  4621.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    3291.96 ms /    24 runs   (  137.16 ms per token,     7.29 tokens per second)\n",
      "llama_print_timings:       total time =    3345.45 ms /    25 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Gandalf mentions that Th√©oden, the king, died and has both honor and peace. /INST]'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"who died\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching examples at: '/home/darthfader/code/medical-chatbots/gradio_cached_examples/245'\n",
      "Caching example 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   19003.12 ms\n",
      "llama_print_timings:      sample time =      10.09 ms /    43 runs   (    0.23 ms per token,  4262.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =   88067.73 ms /  1788 tokens (   49.25 ms per token,    20.30 tokens per second)\n",
      "llama_print_timings:        eval time =    5741.56 ms /    42 runs   (  136.70 ms per token,     7.32 tokens per second)\n",
      "llama_print_timings:       total time =   93921.05 ms /  1830 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching example 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   19003.12 ms\n",
      "llama_print_timings:      sample time =      38.64 ms /   152 runs   (    0.25 ms per token,  3933.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =   95143.47 ms /  1781 tokens (   53.42 ms per token,    18.72 tokens per second)\n",
      "llama_print_timings:        eval time =   21063.89 ms /   151 runs   (  139.50 ms per token,     7.17 tokens per second)\n",
      "llama_print_timings:       total time =  116641.77 ms /  1932 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching example 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   19003.12 ms\n",
      "llama_print_timings:      sample time =      23.53 ms /    96 runs   (    0.25 ms per token,  4079.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =   95885.94 ms /  1787 tokens (   53.66 ms per token,    18.64 tokens per second)\n",
      "llama_print_timings:        eval time =   13601.06 ms /    95 runs   (  143.17 ms per token,     6.98 tokens per second)\n",
      "llama_print_timings:       total time =  109753.57 ms /  1882 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7874\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7874/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   19003.12 ms\n",
      "llama_print_timings:      sample time =      64.20 ms /   255 runs   (    0.25 ms per token,  3972.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   93528.68 ms /  1781 tokens (   52.51 ms per token,    19.04 tokens per second)\n",
      "llama_print_timings:        eval time =   37174.33 ms /   254 runs   (  146.36 ms per token,     6.83 tokens per second)\n",
      "llama_print_timings:       total time =  131451.24 ms /  2035 tokens\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def predict(message, history):\n",
    "    history = []\n",
    "    for human, assistant in history:\n",
    "        history.append({\"role\": \"user\", \"content\": human})\n",
    "        history.append({\"role\": \"assistant\", \"content\": assistant})\n",
    "    history.append({\"role\": \"user\", \"content\": message})\n",
    "    response = query_engine.query(str(history))\n",
    "    with open(\n",
    "        \"user_queries.txt\",\n",
    "        \"a\",\n",
    "        encoding=\"utf-8\",\n",
    "    ) as f:\n",
    "        f.write(\"q: \" + message + \" : \" + \"{}\".format(response.response) + \"\\n\")\n",
    "    return str(response.response)\n",
    "\n",
    "\n",
    "gr.ChatInterface(\n",
    "    predict,\n",
    "    textbox=gr.Textbox(\n",
    "        placeholder=\"Ask me a question about the LOTR\", container=False, scale=7\n",
    "    ),\n",
    "    title=\"Ilya horyas men car√´ √∫vi√´ n√° i car√´ l√∫menen yan me n√° antaina\",\n",
    "    description=\"Ask the bot stuff. Follow-up questions do not work, new question every time is what works. It's not chatGPT. and like me it sometimes stops mid sentence.\",\n",
    "    theme=\"soft\",\n",
    "    examples=[\"What food is mentioned in LOTR?\", \"Are Hobbits Vegan?\", \"What's the deal with the blue wizards?\"],\n",
    "    cache_examples=True,\n",
    "    retry_btn=\"Re-generate\",\n",
    "    undo_btn=\"Delete Previous\",\n",
    "    clear_btn=\"Clear\",\n",
    ").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "311-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
